\documentclass[twoside,11pt]{article}
\usepackage{apalike}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

%\usepackage{jmlr2e}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\newcommand{\url}[1]{{\tt #1}}

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

%\jmlrheading{1}{2000}{1-48}{4/00}{10/00}{Wray Buntine}

% Short headings should be running head and authors last names

%\ShortHeadings{hca for Topic Models}{Buntine}
%\firstpageno{1}

\begin{document}

\title{hca: Tool for Multicore Non-parametric Topic Models}

\author{Wray Buntine, {\tt wray.buntine@monash.edu} \\
       Faculty of Information Technology \\
       Monash University\\
       Clayton, VIC, 3800, Australia}

%\editor{Enid Blighton}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
This paper describes {\tt hca}, an open source command-line tool to
train and test topic models.  The tool implements a variety of Gibbs
samplers for non-parametric models using an efficient multicore
implementation
of hierarchical Pitman-Yor processes.  These are used for both the
document-topic component and the topic-word component, and to model
burstiness of words in topics.  Various diagnostics, document
completion testing and coherence measurements with PMI are also
supported.  The package consists of a main command-line tool
and a set of support utilities. The documentation includes a user's
guide with a mini tutorial.
\end{abstract}

\paragraph{Keywords:}
  topic model, hierarchical Pitman-Yor process, non-parametric Bayesian model,
  Gibbs sampling
%\end{keywords}

\section{Introduction}
Topic models are a form of non-negative matrix factorisation
and some versions also correspond to normalised independent
(multinomial) components.
They were originally developed
as a Bayesian variant of probabilistic latent semantic analysis.
The early model, Latent Dirichlet Allocation (LDA),
used a simple symmetric Dirichlet prior for columns of the
document-topic component and rows of the topic-word component, but
researchers soon realised non-symmetric priors could
improve modelling performance \cite{wallach2009rethinking}.
Using a Dirichlet process (DP) as a prior
for columns of the document-topic component was proposed by
\cite{TehJorBea2006}, and Teh distributed a relatively robust Gibbs
sampler for the task \cite{TehNBMM21}.  While this
purportedly allows the
model to ``pick'' the right number of topics, more importantly,
it allows some topics to be rarer than others.
In the simple symmetric model of LDA,
all topics are assumed equally likely {\it apriori}.

Over the subsequent decade many
research efforts have attempted to improve on Teh's algorithm,
using methods including collapsed Gibbs
samplers, approximate fitting and variational methods.
Others have developed alternative extensions to LDA.
\cite{Sato:2010} have also placed a Pitman-Yor process (PYP) as a prior
for the rows of the topic-word component.
The common prior acts like a ``background'' topic: it alleviates the need
for non-topical words (like stop words)
to be modelled by a single topic because they are
shared across all topics.
\cite{Doyle:2009} have modelled rows of the topic-word component using
a Dirichlet Compound multinomial, rather than a multinomial,
in order to introduce ``burstiness,'' whereby some words in
a document are encouraged to appear multiple times (in a ``burst'').
This tends to produce models with less topics,
and  improves perplexity.
This is implemented with a PYP, yielding a small factor overhead
in space and time over the standard collapsed Gibbs sampler.

While there are numerous other improvements made to vanilla topic
modelling,
for instance using an Indian buffet process (IBP) on both
component matrices to sparsify them, from \cite{archambeau2015latent},
these above are the ones reproduced in the tool {\tt hca}:
\begin{itemize}
\item
  DPs and PYPs on both
  the document-topic component and the topic-word component,
\item
  burstiness via an additional document specific PYP on
  topic-word component, and
\item
  hyperparameter sampling
  by default using adaptive rejection sampling of
  \cite{gilks1992adaptive}.
\end{itemize}
Results using the tool were presented at KDD 2014 by
\cite{buntinemishra14}.
The implementation uses the table indicator sampling method of
\cite{chen2011sampling} which is a collapsed version of
Teh's sampler but requires just a small factor in
space and time overhead over the traditional
collapsed sampler for LDA of \cite{griffiths2004finding}.
This collapses the sizes of the tables in the classic
hierarchical Chinese restaurant samplers, leaving just the
count of the number of tables.
Most importantly, it uses no dynamic memory like the Chinese restaurant
samplers.

\section{The {\tt hca} Commandline Tool}

The {\tt hca} tool is distributed on the MPL~2.0 license
and is implemented as a Unix style command line tool
with multiple options for tasks including
specifying a variety of different non-parametric topic models,
training and testing models,
reporting diagnostics, initialising and controlling fitting of
hyperparameters, restarts and checkpointing.
These options are all documented in a standard Unix style man page with
duplicate PDF version.  Moreover, further examples of using the tool
are given at the end of the man page.

The tool itself is implemented in C.  A multicore version
can be compiled which uses threads and atomic operations available in C11
to keep data consistent. This gets, for instance, about 5 times speed-up
with 8 cores, and uses little memory duplication so can work with
quite large data sets (gigabytes of text).
In our experience, however, it does not work well with more cores like 32.
The code is self-contained, with no library dependencies.
Source code for tasks such as
adaptive rejection sampling, and Gamma variable sampling,
have been included from existing open source libraries
to eliminate dependencies.
Compilation and testing has been successful on Mac OSX and a variety of Unices,
and single-core compilation has been done under Cygwin.

While the development repository is in Github\footnote{{\tt https://github.com/wbuntine/topic-models} and {\tt https://github.com/wbuntine/libstb}},
the best way to obtain the code is via MLOSS\footnote{\tt http://www.mloss.org/software/view/527/} because this combines the two components.

\section{Using the System}

While the tool accepts many different data formats, the
most common is a format sometimes referred to as the LDA-C format.
This is a sparse matrix format where each line represents a document,
and the line contains a list of ``index:count'' pairs giving the
non-zero entries of the document by word matrix.  An example dataset
(``ch.ldac'' and its companion ``ch.tokens'' listing the words
corresponding to the indices) is included in the release.
This is composed of 400 news articles about church, ``bagged up''
as a data product from \cite{Lewis:2004}.
A typical sequence of commands is:
\begin{verbatim}
   hca -v -C200 -K40 -q6 data/ch /tmp/C1
   hca -v -v -V -r0 -C0 -orat,10 -e data/ch /tmp/C1
\end{verbatim}
The first line builds a 40-topic model with 300 Gibbs cycles
using the default configuration
for {\tt hca}, run on 6 cores in 12 seconds.
The default configuration has a DP on the document-word component,
a PYP on the topic-word component,
both with a GEM at their root, and does no burstiness modelling.
The second lines reports on the learnt model and parameters
giving the top ten words per topic.
For instance, the PYP on the topic-word
component settled on a discount of 0.31 and concentration of 209,
and the DP on the document-word component settled on a concentration of 0.8.
The three most populous topics have significant words that show the
era of the articles (1986-1987):
\begin{verbatim}
   teresa,missionaries,nirmala,calcutta,nursing,respirator
   diana,parker,bowles,camilla,charles,divorce
   appendix,crucitti,tumour,parkinson,trembling,pope
\end{verbatim}
More examples of the system, for instance document completion
testing \cite{Buntine09},
MCMC estimates of the component matrices,
and diagnostic reports of the generated topics
are given in the document.


\section{Related Software}

A variety of software for HDP-LDA, which places a
hierarchical DP on the document-topic component,
were compared with {\tt hca} and reported in
\cite{buntinemishra14}.
Not all software is distributed, so some comparisons
were done by duplicating experiments.
Performance can be evaluated in terms of
test perplexity, and in terms of computational
speed.  However, different authors make subtle
distinctions in their evaluation of perplexity so
comparisons are treacherous.
For instance, for document completion one may
hold out every 4-th word of a document or the
final quarter of a document.  Since documents
vary in topic across paragraphs, this difference can be
significant.
Moreover, some authors do a poor choice of
hyperparameters, sometimes not fitting them,
so reported results are not always
significant.

{\tt Mallet} \cite{McCallumMALLET} has a asymmetric-symmetric
LDA mode that behaves like a truncated HDP-LDA approximation.
This is significantly faster than all others, and also supports
multicore.  
However, it uses memory duplication with multicore so is not
able to run on larger data sets. 
Similarly, {\tt hca} is a factor faster than all others,
except {\tt Mallet}, and it supports multicore but without
significant memory overhead.
It is much more efficient in memory, partly due
to its implementation in C:  a factor of 3 compared to
{\tt onlinehdp} \cite{WangPB:AISTATS11}, and a factor of 5 or so compared to
{\tt HDP} \cite{TehNBMM21}.
The variational approximation of \cite{Sato:2010} yields
good performance in perplexity, comparable to {\tt HDP} and {\tt Mallet}.
Of the HDP-LDA implementations, however, {\tt hca} was
significantly the best in terms of perplexity.

No other distributed implementations of topic modeling software
perform well on nonparametric modelling of
the topic-word component.
An approximate implementation in {\tt Mallet} reported negative
experiments \cite{wallach2009rethinking}.
For {\tt hca}, nonparametric modelling of
the topic-word component usually significantly improves
perplexity, and the root topic has a beautiful interpretation
as a background topic that picks up non-topical words.

Note, \cite{archambeau2015latent} model sparsity with an
IBP.  Their FTM models sparsity on the
document-topic component,
and their LIDA models sparsity in both document-topic and
topic-word components.
They report good results
compared with Teh's {\tt HDP}, although they used a fixed
setting for the concentration of the DP so it is
not clear if the results would hold with proper fitting.
Attempting to reproducing their results using
{\tt hca} with the same data sets yields
the following results for log of the test-set perplexity.

\begin{tabular}{l|ll}
Data & KOS & NIPS \\\hline
FTM (3-par) & 7.266$\pm$0.009 & 6.883$\pm$0.008 \\
LIDA & 7.257$\pm$0.010 & 6.795$\pm$0.007 \\\hline
HPD-LDA ({\tt hca})& 7.253$\pm$0.003 & 6.792$\pm$0.002  \\
NP-LDA  ({\tt hca})& 7.156$\pm$0.003 & 6.722$\pm$0.003
\end{tabular}

\noindent
In these results, 
NP-LDA has a DP on the document-topic component
and a PYP on the topic-word component.

In summary, in terms of shear speed and software support,
{\tt Mallett} is a clear winner.  When high performance
in terms of perplexity is required, and
with a fully non-parametric implementation,
{\tt hca} offers an alternative.

\section*{Acknowledgements}
{Some parts of the software were co-authored by, and benefited greatly
  from discussions with Dr.\ Lan Du and Swapnil Mishra.
  The diagnostics were developed with consultation from Christopher Ball of
  Metaheuristica.
  The software was developed at NICTA Canberra and Monash University,
  based on earlier infrastructure developed at
  Helsinki Institute for Information Technology.
}

\bibliographystyle{apalike}
\bibliography{hca}

\end{document}
