\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

\jmlrheading{1}{2000}{1-48}{4/00}{10/00}{Wray Buntine}

% Short headings should be running head and authors last names

\ShortHeadings{hca for Topic Models}{Buntine}
\firstpageno{1}

\begin{document}

\title{hca: Tool for Multicore Non-parametric Topic Models}

\author{\name Wray Buntine \email wray.buntine@monash.edu \\
       \addr Faculty of Information Technology \\
       Monash University\\
       Clayton, VIC, 3800, Australia}

\editor{Enid Blighton}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
This paper describes {\tt hca}, an open source command-line tool to
train and test topic models.  The tool implements a variety of Gibbs
samplers for non-parametric models using an efficient implementation
of hierarchical Pitman-Yor processes.  These are used for both the
document-topic component and the topic-word component, and to model
burstiness of words in topics.  Various diagnostics, document
completion testing and coherence measurements with PMI are also
supported.  This is also the first multicore non-parametric topic
model distribution.  The package consists in a main command-line tool
and a set of support utilities. The documentation includes a user's
guide with a mini tutorial.
\end{abstract}

\begin{keywords}
  topic model, hierarchical Pitman-Yor process, non-parametric Bayesian model,
  Gibbs sampling
\end{keywords}

\section{Introduction}
Topic models are a variant of non-negative matrix factorisation developed
as a Bayesian variant of probabilistic latent semantic analysis.  Early models
used a simple symmetric Dirichlet prior for columns of the
document-topic component and rows of the topic-word component, but
researchers soon realised non-symmetric priors could
improve modelling performance.  Using a Dirichlet process as a prior
for columns of the document-topic component was proposed by
\cite{TehJorBea2006}, and Teh distributed a relatively robust Gibbs
sampler for the task \cite{TehNBMM21}.

Over the subsequent decade many different
research efforts have presented different algorithms including Gibbs
samplers, approximate fitting and variational methods.
\cite{Sato:2010} have also placed a Pitman-Yor process as a prior
for the rows of the topic-word component.
\cite{Doyle:2009} have modelled rows of the topic-word component using
a Dirichlet Compound multinomial, rather than a multinomial,
in order to introduce ``burstiness,'' whereby some words in
a document are encouraged to appear multiple times (in a ``burst'').
This tends to produce models with less topics,
and dramatically improves perplexity.
While there are numerous other improvements made to vanilla topic
modelling, these are the ones we reproduce in our tool {\tt hca}.
The hyperparameters for the nonparametric models are also
fit by default using adaptive rejection sampling by
\cite{gilks1992adaptive}.

\section{The {\tt hca} Commandline Tool}

The {\tt hca} tool is implemented as a Unix style command line tool
with multiple options for different tasks including
specifying a variety of different non-parametric topic models,
training and testing models,
reporting diagnostics, initialising and controlling fitting of
hyperparameters, restarts and checkpointing.
These options are all documented in a standard Unix style man page with
duplicate PDF version.  Moreover, any examples of using the tool
are given at the end of the man page.

The tool itself is implemented in C.  A multicore version
can be compiled which uses threads and atomic operations to
keep data consistent. This gets, for instance, about 5 times speed-up
with 8 cores, and uses little memory duplication so can work with
quite data sets (gigabytes of text).  The code is self-contained,
requiring no library dependencies.  Routines for tasks such as
adaptive rejection sampling, and Gamma variable sampling,
have been included from existing open source libraries.
Compilation has been successful on Mac OSX and a variety of Unicies,
and single-core compilation has been done under Cygwin.

Some utilities available with the tool are coded in Perl.
However, these are not essential for operation.

\section{Using the System}


\acks{Some parts of the software were co-authored by, and benefited greatly
  from discussions with Lan Du and Swapnil Mishra.  The software was developed at NICTA Canberra and Monash University. }

\bibliography{hca}

\end{document}
