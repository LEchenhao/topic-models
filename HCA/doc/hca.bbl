\begin{thebibliography}{}

\bibitem[Archambeau et~al., 2015]{archambeau2015latent}
Archambeau, C., Lakshminarayanan, B., and Bouchard, G. (2015).
\newblock Latent {IBP} compound {D}irichlet allocation.
\newblock {\em Pattern Analysis and Machine Intelligence, IEEE Transactions
  on}, 37(2):321--333.

\bibitem[Buntine, 2009]{Buntine09}
Buntine, W. (2009).
\newblock Estimating likelihoods for topic models.
\newblock In {\em Proceedings of the 1st Asian Conference on Machine Learning},
  Nanjing, China.

\bibitem[Buntine and Mishra, 2014]{buntinemishra14}
Buntine, W. and Mishra, S. (2014).
\newblock Experiments with non-parametric topic models.
\newblock In {\em 20th ACM SIGKDD Conf.\ on Knowledge Discovery and Data
  Mining}, page 881â€“890.

\bibitem[Chen et~al., 2011]{chen2011sampling}
Chen, C., Du, L., and Buntine, W. (2011).
\newblock Sampling table configurations for the hierarchical
  {P}oisson-{D}irichlet process.
\newblock In {\em Machine Learning and Knowledge Discovery in Databases:
  European Conference, ECML PKDD}, pages 296--311. Springer.

\bibitem[Doyle and Elkan, 2009]{Doyle:2009}
Doyle, G. and Elkan, C. (2009).
\newblock Accounting for burstiness in topic models.
\newblock In {\em Proc.\ of the 26th Annual Int.\ Conf.\ on Machine Learning},
  ICML '09, pages 281--288.

\bibitem[Gilks and Wild, 1992]{gilks1992adaptive}
Gilks, W.~R. and Wild, P. (1992).
\newblock Adaptive rejection sampling for {G}ibbs sampling.
\newblock {\em Applied Statistics}, pages 337--348.

\bibitem[Griffiths and Steyvers, 2004]{griffiths2004finding}
Griffiths, T. and Steyvers, M. (2004).
\newblock Finding scientific topics.
\newblock {\em Proceedings of the National Academy of Sciences}, 101(suppl
  1):5228--5235.

\bibitem[Lewis et~al., 2004]{Lewis:2004}
Lewis, D., Yang, Y., Rose, T., and Li, F. (2004).
\newblock {RCV1}: A new benchmark collection for text categorization research.
\newblock {\em J. Mach. Learn. Res.}, 5.

\bibitem[Li et~al., 2014a]{LiAhmed14}
Li, A., Ahmed, A., Ravi, S., and Smola, A. (2014a).
\newblock Reducing the sampling complexity of topic models.
\newblock In {\em ACM Conference on Knowledge Discovery and Data Mining (KDD)}.

\bibitem[Li et~al., 2014b]{LiAnd14}
Li, M., Anderson, D., Park, J., Smola, A., Ahmed, A., Josifovski, V., Long, J.,
  Shekita, E., and Su, B.-Y. (2014b).
\newblock Scaling distributed machine learning with the parameter server.
\newblock In {\em Operating Systems Design and Implementation (OSDI)}, pages
  583--598.

\bibitem[McCallum, 2002]{McCallumMALLET}
McCallum, A.~K. (2002).
\newblock Mallet: A machine learning for language toolkit.
\newblock http://mallet.cs.umass.edu.

\bibitem[Sato and Nakagawa, 2010]{Sato:2010}
Sato, I. and Nakagawa, H. (2010).
\newblock Topic models with power-law using {P}itman-{Y}or process.
\newblock In {\em 16th ACM SIGKDD Conf.\ on Knowledge Discovery and Data
  Mining}, pages 673--682. ACM.

\bibitem[Teh, 2004]{TehNBMM21}
Teh, Y.~W. (2004).
\newblock Nonparametric {B}ayesian mixture models - release 2.1.
\newblock \url{http://www.stats.ox.ac.uk/\~teh/software.html}.
\newblock MATLAB and C code [Online; accessed 18 {M}arch 2016].

\bibitem[Teh et~al., 2006]{TehJorBea2006}
Teh, Y.~W., Jordan, M.~I., Beal, M.~J., and Blei, D.~M. (2006).
\newblock Hierarchical {D}irichlet processes.
\newblock {\em Journal of the ASA}, 101(476):1566--1581.

\bibitem[Wallach et~al., 2009]{wallach2009rethinking}
Wallach, H., Mimno, D., and McCallum, A. (2009).
\newblock Rethinking {LDA}: Why priors matter.
\newblock In {\em Advances in Neural Information Processing Systems 19}.

\bibitem[Wang et~al., 2011]{WangPB:AISTATS11}
Wang, C., Paisley, J., and Blei, D. (2011).
\newblock Online variational inference for the hierarchical {D}irichlet
  process.
\newblock In {\em AISTATS '11}.

\end{thebibliography}
